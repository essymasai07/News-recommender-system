{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 90\u001b[0m\n\u001b[1;32m     88\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[1;32m     89\u001b[0m     schedule\u001b[38;5;241m.\u001b[39mrun_pending()\n\u001b[0;32m---> 90\u001b[0m     \u001b[43mtime\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msleep\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import feedparser\n",
    "import csv\n",
    "import json\n",
    "import schedule\n",
    "import time\n",
    "from datetime import datetime\n",
    "import os\n",
    "\n",
    "# Function to fetch articles from RSS feeds\n",
    "def fetch_rss_feeds(feed_urls):\n",
    "    all_articles = []\n",
    "    for url in feed_urls:\n",
    "        print(f\"Fetching from: {url}\")\n",
    "        feed = feedparser.parse(url)\n",
    "        \n",
    "        if feed.bozo:  # Check if there's a problem parsing the feed\n",
    "            print(f\"Error parsing feed: {url}\")\n",
    "            continue\n",
    "        \n",
    "        print(f\"Found {len(feed.entries)} articles in {url}\")\n",
    "        \n",
    "        # Extract articles\n",
    "        for entry in feed.entries:\n",
    "            all_articles.append({\n",
    "                \"source\": feed.feed.title if 'title' in feed.feed else 'Unknown',\n",
    "                \"title\": entry.title,\n",
    "                \"link\": entry.link,\n",
    "                \"summary\": entry.summary if 'summary' in entry else '',\n",
    "                \"published\": entry.published if 'published' in entry else 'Unknown'\n",
    "            })\n",
    "    return all_articles\n",
    "\n",
    "# Function to remove duplicates based on article link\n",
    "def remove_duplicates(new_articles, existing_articles):\n",
    "    existing_links = set(article['link'] for article in existing_articles)\n",
    "    unique_articles = [article for article in new_articles if article['link'] not in existing_links]\n",
    "    return unique_articles\n",
    "\n",
    "# Function to save articles to CSV\n",
    "def save_to_csv(articles, filename=\"news_data.csv\"):\n",
    "    if not articles:\n",
    "        print(\"No new articles to save!\")\n",
    "        return\n",
    "    \n",
    "    # Check if the file already exists\n",
    "    if os.path.exists(filename):\n",
    "        # Read existing data\n",
    "        with open(filename, 'r', encoding='utf-8') as f:\n",
    "            existing_articles = list(csv.DictReader(f))\n",
    "    else:\n",
    "        # If the file doesn't exist, start fresh\n",
    "        existing_articles = []\n",
    "\n",
    "    # Remove duplicates\n",
    "    unique_articles = remove_duplicates(articles, existing_articles)\n",
    "\n",
    "    # Save new unique articles\n",
    "    with open(filename, 'a', newline='', encoding='utf-8') as f:\n",
    "        writer = csv.DictWriter(f, fieldnames=articles[0].keys())\n",
    "        # Write header only if the file is new\n",
    "        if not existing_articles:\n",
    "            writer.writeheader()\n",
    "        writer.writerows(unique_articles)\n",
    "    \n",
    "    print(f\"Saved {len(unique_articles)} new articles to {filename}\")\n",
    "\n",
    "# Function to fetch and save articles every hour\n",
    "def fetch_and_save():\n",
    "    print(f\"Fetching news articles at {datetime.now()}\")\n",
    "    rss_feed_urls = [\n",
    "        \"http://feeds.bbci.co.uk/news/rss.xml\",\n",
    "        \"http://rss.cnn.com/rss/cnn_topstories.rss\",\n",
    "        \"https://www.theguardian.com/uk/rss\",\n",
    "        \"https://rss.nytimes.com/services/xml/rss/nyt/HomePage.xml\"\n",
    "    ]\n",
    "    \n",
    "    # Fetch new articles\n",
    "    news_articles = fetch_rss_feeds(rss_feed_urls)\n",
    "    \n",
    "    # Save to CSV and remove duplicates\n",
    "    save_to_csv(news_articles)\n",
    "\n",
    "# Schedule the task every hour\n",
    "schedule.every(1).hour.do(fetch_and_save)\n",
    "\n",
    "# Run the scheduler\n",
    "if __name__ == \"__main__\":\n",
    "    while True:\n",
    "        schedule.run_pending()\n",
    "        time.sleep(1)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "newenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
